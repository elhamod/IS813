{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLsz9g5Hc/XQw1mjCPVhJZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS813/blob/main/Week2/IS813_2024_Week2_pre_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IS813 Week 2: Advanced Language Modeling\n"
      ],
      "metadata": {
        "id": "Z4FDekTFJuaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. use Google Colab for this assignment.\n",
        "\n",
        "2. **You are NOT allowed to use ChatGPT for this assignment. However, you may use Google and other online resources. As per the syllabus, you are required to cite your usage. You are also responsible for understanding the solution and defending it when asked in class.**\n",
        "\n",
        "3. For each question, fill in the answer in the cell(s) right below it. The answer could be code or text. You can add as many cells as you need for clarity.\n",
        "\n",
        "4. Enter your BUID (only numerical part) below.\n",
        "\n",
        "5. **Your submission on Blackboard should be the downloaded notebook (i.e., ipynb file). It should be prepopulated with your solution (i.e., the TA and/or instructor need not rerun the notebook to inspect the output). The code, when executed by the TA and/or instructor, should run with no runtime errors.**"
      ],
      "metadata": {
        "id": "p7KF7bFqKEIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Pre-class Work"
      ],
      "metadata": {
        "id": "F95dcB4zKOnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Setup"
      ],
      "metadata": {
        "id": "_ic8VY0SNXX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install some important HuggingFace packages"
      ],
      "metadata": {
        "id": "7QxJrXiOXgP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL8sYzUuXpR8",
        "outputId": "6ae3fc7b-5649-4081-88fd-75ba61f3a392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdoGs8-UeOab"
      },
      "outputs": [],
      "source": [
        "BUID = 123456 #e.g., 123456 ONLY NUMERICAL PART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssLsZbOeOac"
      },
      "source": [
        " Machine learning is generally stochastic, meaning you get different results for different runs. To avoid that, you can \"seed\" your code. This code uses your BU id (only the numeric part) as a seed for all random number generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3djGT1QeOac"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import set_seed\n",
        "\n",
        "# Set a seed for the built-in Python random module\n",
        "random.seed(BUID)\n",
        "# Set a seed for NumPy\n",
        "np.random.seed(BUID)\n",
        "# Set a seed for HuggingFace\n",
        "set_seed(BUID)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Using a Pre-trained GPT Model"
      ],
      "metadata": {
        "id": "jAbNAfRIKXrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.1 Complete the Sentence..."
      ],
      "metadata": {
        "id": "cXzhi8BwId5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get our feet wet by loading a GPT2 model and using it to generate some text based on a prompt. You may want to refer to [this webpage](https://huggingface.co/openai-community/gpt2) for help. **(10 Points)**\n",
        "\n",
        "- You will generate completions for two prefixes:\n",
        "  - \"Damascus is a\"\n",
        "  - \"Barcelona is a\"\n",
        "- For each prefix, generate 10 completions.\n",
        "- Limit the maximum of length of each completion to 20 *tokens*."
      ],
      "metadata": {
        "id": "FEar0ym7MHji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "### Create a GPT2 generator pipeline\n",
        "\n",
        "### Generate the answer to the question \"Damascus is a\"\n"
      ],
      "metadata": {
        "id": "7kCqDW26MXCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate the answer to the question \"Barcelona is a\""
      ],
      "metadata": {
        "id": "tNiwlZ9xPYKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Reflective Questions"
      ],
      "metadata": {
        "id": "Fj3_U-QZSauf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Based on our former class discussions and material, what is the explanation for GPT2's ability of generating a diverse set of completions from the same prefix? **(5 Points)**\n",
        "\n",
        "2. What do you notice about the generated texts for the two prompts? Any interesting commonalities or stark differences? What is the underlying explanation for your observations? **(10 Points)**\n",
        "\n",
        "3. Based on our former class discussions and material, what are the underlying set of steps the model is taking to generate the completions, given the prefix? **(5 Points)**"
      ],
      "metadata": {
        "id": "8slB7-AcSjvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "*Leave your answer here*"
      ],
      "metadata": {
        "id": "IbyxZbcVUON_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3 Using OpenAI API"
      ],
      "metadata": {
        "id": "P4sXkvRTLxUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3.1 Install OpenAI package"
      ],
      "metadata": {
        "id": "M9HbjkFH0McU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKLoIDLh0XYq",
        "outputId": "976988bc-7453-42a1-f7d6-25537511b3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.43.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.43.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.3.2 Generate Text with OpenAI API."
      ],
      "metadata": {
        "id": "5ckx72mp0RQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have experimented with loading a language model (GPT2) *locally* and using it to generate some sentences, how about we instead use someone else's model through an API? Let's experiment with OpenAI's API!"
      ],
      "metadata": {
        "id": "HGzx2LL7LszE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In order to use OpenAI API, you first need to get an API key that allows you to use the class's OpenAI resources. You can create the key through [this link](https://platform.openai.com/api-keys) after signing in.\n",
        "- Once you have created the key, you will save it as a secret in Google Colab. See [this example](https://drlee.io/how-to-use-secrets-in-google-colab-for-api-key-protection-a-guide-for-openai-huggingface-and-c1ec9e1277e0) for how to store and load the API key. For grading purposes, you MUST name your key *MyOpenAIKey*.\n",
        "- Now, you are set! Use [OpenAI API documentation](https://platform.openai.com/docs/guides/text-generation) to complete the same two prefixes in 1.2. [This webpag](https://platform.openai.com/docs/api-reference/chat/create)e may also be helpful. **(10 Points)**\n",
        "  - You must use the *gpt-4o-mini* model.\n",
        "  - Similar to 1.2, you will generate up to 20 tokens per request.\n",
        "  - Similar to 1.2, you will generate 10 different completions.\n",
        "  - You will set the seed to be your BUID.\n",
        "  - Make sure the API call *completes* the given prefix (i.e., it does not start a new sentence)."
      ],
      "metadata": {
        "id": "tjSt3_mBrohm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1JSEA03Jb9M"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "### Load your API Key\n",
        "\n",
        "\n",
        "\n",
        "### Request the answer to the question \"Damascus is a\"\n",
        "\n",
        "\n",
        "### Print all 10 completions:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Request the answer to the question \"Barcelona is a\"\n",
        "\n",
        "\n",
        "### Print all 10 completions:\n"
      ],
      "metadata": {
        "id": "5aPZ5GDVz3Nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Reflective Questions"
      ],
      "metadata": {
        "id": "t3tuOvFaIZHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What do you notice about the generated texts for the two prompts? Any interesting commonalities or stark differences? **(5 Points)**\n",
        "\n",
        "2. How do the results in 1.3.2 compare to those from 1.2.1? What do you think is the underlying cause? **(5 Points)**\n"
      ],
      "metadata": {
        "id": "HQpqlQcPIUT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "*Leave your answers here.*"
      ],
      "metadata": {
        "id": "uVp056iam8DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: In-class Work"
      ],
      "metadata": {
        "id": "FWtanyk2ODGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Training a model from scratch"
      ],
      "metadata": {
        "id": "JPpZIA28OFUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In 1.2, we saw how we could load a pre-trained model and use it to complete sentences. Now, let's see how we could train our own model."
      ],
      "metadata": {
        "id": "iLvpksYZOMuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Loading an untrained model"
      ],
      "metadata": {
        "id": "wjuP6HopOYkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load an untrained GPT2 model. Take a look at [this documentation](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config). Generate a maximum of 20 tokens per completion."
      ],
      "metadata": {
        "id": "D-1qiorqOcL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "### Initializing a GPT2 configuration.\n",
        "\n",
        "\n",
        "### Initializing an untrained model using the configuration.\n",
        "\n",
        "\n",
        "### Load a GPT2 tokenizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bqcFJpjFOf1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Damascus is a\"\n",
        "\n",
        "\n",
        "### Tokenize the prompt\n",
        "\n",
        "\n",
        "### Generate the completion as tokens\n",
        "\n",
        "\n",
        "### Print the result after converting it back to text.\n"
      ],
      "metadata": {
        "id": "F48z2AOrQQIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1.2 Reflective questions\n",
        "\n"
      ],
      "metadata": {
        "id": "o2GpLP26Tu32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- How could you verify that the model is untrained?"
      ],
      "metadata": {
        "id": "zB1QtqycWplT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Training the model"
      ],
      "metadata": {
        "id": "XGZO3Ul4ObmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train the model on some corpus about Damascus."
      ],
      "metadata": {
        "id": "-ghVYxEaT8mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"Damascus, the capital of Syria, is one of the oldest continuously inhabited cities in the world, offering a blend of history, culture, and charm. Its ancient streets are lined with historical landmarks, from the grand Umayyad Mosque to the Citadel, where layers of history from different civilizations can be traced. The Old City, with its narrow alleyways and bustling souks, provides a window into the city's rich past, where traders, artisans, and visitors alike mingle in a timeless setting. The scent of jasmine and citrus trees, which dot the courtyards of traditional Damascene houses, adds to the city's allure, making every corner feel like a step back in time.\n",
        "\n",
        "Beyond its historical significance, Damascus is known for its hospitality and warmth. Locals welcome visitors with open arms, eager to share their stories and offer traditional Syrian delights like shawarma, kibbeh, and baklava. The city's cafés, where people gather over tea and coffee, offer a relaxed atmosphere, making it easy to soak in the daily rhythm of life. From the bustling Hamidiyeh Bazaar to the quieter, tucked-away cafés in the Old City, Damascus offers a unique mix of old and new, where centuries of history coexist with modern life.\n",
        "\n",
        "Damascus is not just a city of the past but one with a thriving, vibrant culture. Its art galleries, music festivals, and poetry readings showcase a lively creative scene that continues to grow, despite the challenges the city has faced. The natural beauty of nearby Mount Qasioun provides a stunning backdrop, especially at sunset when the city is bathed in a golden glow. Whether strolling through its ancient streets or enjoying the breathtaking views, visitors are sure to be captivated by the beauty, resilience, and charm of Damascus.\"\"\""
      ],
      "metadata": {
        "id": "hD6aGQeLU7w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "### Initializing a untrained model using the configuration.\n",
        "trained_model = GPT2LMHeadModel(configuration)\n",
        "\n",
        "### We need a collator to calculate the loss\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# This function create a dataset as subsequences of the paragraph\n",
        "def create_subsequences(text, max_length=512):\n",
        "    input_ids = tokenizer(text, return_tensors='pt', add_special_tokens=False).input_ids[0]\n",
        "\n",
        "    sequences = []\n",
        "    for start_idx in range(len(input_ids)):\n",
        "        # Slice the input_ids to create a sequence starting at each token\n",
        "        # sequence = input_ids[start_idx- max_length+1:start_idx+1 ]\n",
        "        sequence = input_ids[start_idx:start_idx+max_length ]\n",
        "        sequences.append({'input_ids': sequence})\n",
        "    return sequences\n",
        "\n",
        "# Create the dataset and tokenize it\n",
        "data = create_subsequences(corpus)\n",
        "tokenized_dataset = Dataset.from_list(data)\n",
        "\n",
        "# Define the training arguments\n",
        "\n",
        "\n",
        "# Set up the Trainer\n",
        "\n",
        "\n",
        "# Train the model\n"
      ],
      "metadata": {
        "id": "s0A36_q6XLFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's generate some completions using the trained model."
      ],
      "metadata": {
        "id": "C7TPEV2dXDyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Damascus\"\n",
        "\n",
        "### Tokenize the prompt\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "\n",
        "### Set model in evaluation  mode.\n",
        "trained_model.eval()\n",
        "\n",
        "### Print the result\n",
        "for i in range(10):\n",
        "  ### Generate the completion as tokens\n",
        "\n",
        "  ### Convert from tokens back to text."
      ],
      "metadata": {
        "id": "FlhhFETfeW4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.2 Introducing Streamlit: Deploying an OpenAI API Interactive App."
      ],
      "metadata": {
        "id": "jvccp4Pue_1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have mastered harnessing OpenAI AP, let's deploy an app that uses it! An example of such an app can be found [here](https://is883openaiapiux.streamlit.app/).\n",
        "\n",
        "Things to keep in mind:\n",
        "\n",
        "- Add the secret API key in their proper place under *Settings*. Refer to [this link](https://docs.streamlit.io/deploy/streamlit-community-cloud/deploy-your-app/secrets-management) for help.\n",
        "- Add the required packages under *requirements.txt* in your GitHub repo.\n"
      ],
      "metadata": {
        "id": "hniSh2rEfuc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Homework"
      ],
      "metadata": {
        "id": "ecrC1JLuOK6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Evaluate The GPT2 Models"
      ],
      "metadata": {
        "id": "jD1wfWsxZe-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After having trained the GPT2 model on your custom paragraph in 2.1.3, can you calculate the perplexity for both the trained and untrained models in 2.1.1 and 2.1.3, respectively, *using the same paragraph*. **(10 Points)**\n",
        "\n",
        "Use this [webpage](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72) for guidance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sQznrnV7ZlDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "### Calculate and print the perplexity of the untrained model.\n",
        "\n",
        "\n",
        "\n",
        "### Calculate and print the perplexity of the trained mode,\n",
        "\n"
      ],
      "metadata": {
        "id": "ghdkvX2FZj-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How do the perplexities compare? What is the underlying justification for this observation? **(5 Points)**\n",
        "2. *Based on the perplexities you calculated*, can you say one of the models is better than the other *as a general purpose text generator*? Explain your reasoning. **(5 Points)**"
      ],
      "metadata": {
        "id": "1b5qxZ5zd-8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "*Leave your answer here.*"
      ],
      "metadata": {
        "id": "xMowSfrvmEFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Deploy a GPT2 App online"
      ],
      "metadata": {
        "id": "LvrEKUXYT6eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, that you have seen how to use Streamlit in 2.2 and how to locally use a GPT2 model in 1.2.1, you should be able to put the two together to create a webpage that takes in your prompt and returns GPT2's response.\n",
        "\n",
        "- Your app should have a textbox that takes in the user's prompt **(5 Points)**\n",
        "- Your app should also have a textbox that takes in the number of tokens to be used (i.e., the expected length of the response). **(5 Points)**\n",
        "- Once the user hits Enter, the webpage should display the response to that prompt.\n",
        "- Using this [documentation](https://huggingface.co/docs/transformers/main/en/generation_strategies), make your app generate two responses: One with a high level of *creativity*, and one that is more *predictable*. **(10 Points)**\n",
        "\n",
        "- Share the URL of your *publicly visible* GitHub so your code could be graded.\n",
        "- Share the URL of your app after deploying it. Make sure it is running so it could be graded."
      ],
      "metadata": {
        "id": "ZM06lOK3asxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**\n",
        "\n",
        "*Leave your answer here*"
      ],
      "metadata": {
        "id": "tZnYLsb4kejh"
      }
    }
  ]
}